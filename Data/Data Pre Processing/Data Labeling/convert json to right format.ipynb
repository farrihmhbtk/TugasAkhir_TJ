{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. Output saved to TJ - self_training//batch 5/spanaste_format.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import re\n",
    "\n",
    "def get_token_indices(text, ann_start, ann_end):\n",
    "    \"\"\"\n",
    "    Given the text and a character span (ann_start, ann_end),\n",
    "    return the token indices as a list:\n",
    "      - For a single-token span: [i]\n",
    "      - For a multi-token span: [i, j, k, ...] (all tokens that fall within the span)\n",
    "    Tokens are obtained by splitting on whitespace.\n",
    "    \"\"\"\n",
    "    tokens = list(re.finditer(r'\\S+', text))\n",
    "    indices = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        t_start, t_end = token.start(), token.end()\n",
    "        # Include token index if the token lies completely within the annotation span.\n",
    "        if t_start >= ann_start and t_end <= ann_end:\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "def process_tweet(item):\n",
    "    # Use the cleaned text if available; otherwise fall back to full_text.\n",
    "    data = item.get(\"data\", {})\n",
    "    tweet_text = data.get(\"cleaned_full_text\") or data.get(\"full_text\", \"\")\n",
    "    tweet_text = tweet_text.strip()\n",
    "\n",
    "    # Combine annotation results from both \"annotations\" and \"drafts\"\n",
    "    combined_results = []\n",
    "    if \"annotations\" in item:\n",
    "        for ann in item[\"annotations\"]:\n",
    "            if \"result\" in ann:\n",
    "                combined_results.extend(ann[\"result\"])\n",
    "    if \"drafts\" in item:\n",
    "        for draft in item[\"drafts\"]:\n",
    "            if \"result\" in draft:\n",
    "                combined_results.extend(draft[\"result\"])\n",
    "    \n",
    "    # Build a dictionary of label items (aspect/opinion) keyed by their id\n",
    "    label_dict = {}\n",
    "    relation_items = []  # To hold relation objects\n",
    "    for res in combined_results:\n",
    "        if res.get(\"type\") == \"relation\":\n",
    "            # For a relation item, record the linked ids and extract polarity.\n",
    "            labels = res.get(\"labels\", [])\n",
    "            polarity = None\n",
    "            for lab in labels:\n",
    "                if lab.startswith(\"sent:\"):\n",
    "                    polarity = lab.split(\"sent:\")[-1]\n",
    "                    break\n",
    "            relation_items.append({\n",
    "                \"from_id\": res.get(\"from_id\"),\n",
    "                \"to_id\": res.get(\"to_id\"),\n",
    "                \"polarity\": polarity\n",
    "            })\n",
    "        elif res.get(\"type\") == \"labels\":\n",
    "            value = res.get(\"value\", {})\n",
    "            # Retrieve the annotation span and text.\n",
    "            ann_start = value.get(\"start\")\n",
    "            ann_end = value.get(\"end\")\n",
    "            ann_text = value.get(\"text\")\n",
    "            # Determine if this is an \"aspect\" or \"opinion\" annotation.\n",
    "            labels_list = value.get(\"labels\", [])\n",
    "            ann_type = None\n",
    "            if \"aspect\" in labels_list:\n",
    "                ann_type = \"aspect\"\n",
    "            elif \"opinion\" in labels_list:\n",
    "                ann_type = \"opinion\"\n",
    "            if ann_type is not None and ann_start is not None and ann_end is not None:\n",
    "                label_dict[res.get(\"id\")] = {\n",
    "                    \"type\": ann_type,\n",
    "                    \"start\": ann_start,\n",
    "                    \"end\": ann_end,\n",
    "                    \"text\": ann_text\n",
    "                }\n",
    "    \n",
    "    used_ids = set()\n",
    "    triplets = []\n",
    "    # First, form triplets using relation items.\n",
    "    for rel in relation_items:\n",
    "        from_id = rel.get(\"from_id\")\n",
    "        to_id = rel.get(\"to_id\")\n",
    "        polarity = rel.get(\"polarity\")\n",
    "        if from_id in label_dict and to_id in label_dict:\n",
    "            aspect_item = label_dict[from_id]\n",
    "            opinion_item = label_dict[to_id]\n",
    "            used_ids.add(from_id)\n",
    "            used_ids.add(to_id)\n",
    "            aspect_indices = get_token_indices(tweet_text, aspect_item[\"start\"], aspect_item[\"end\"])\n",
    "            opinion_indices = get_token_indices(tweet_text, opinion_item[\"start\"], opinion_item[\"end\"])\n",
    "            triplets.append((aspect_indices, opinion_indices, polarity))\n",
    "    \n",
    "    # Next, for any unpaired annotations, try to pair one aspect with one opinion.\n",
    "    unpaired_aspects = []\n",
    "    unpaired_opinions = []\n",
    "    for id_key, ann in label_dict.items():\n",
    "        if id_key not in used_ids:\n",
    "            if ann[\"type\"] == \"aspect\":\n",
    "                unpaired_aspects.append(ann)\n",
    "            elif ann[\"type\"] == \"opinion\":\n",
    "                unpaired_opinions.append(ann)\n",
    "    # Sort unpaired items by their start position.\n",
    "    unpaired_aspects.sort(key=lambda x: x[\"start\"])\n",
    "    unpaired_opinions.sort(key=lambda x: x[\"start\"])\n",
    "    # Pair them by order. Here we assign a default polarity of \"POS\" (as in your example).\n",
    "    for aspect_item, opinion_item in zip(unpaired_aspects, unpaired_opinions):\n",
    "        aspect_indices = get_token_indices(tweet_text, aspect_item[\"start\"], aspect_item[\"end\"])\n",
    "        opinion_indices = get_token_indices(tweet_text, opinion_item[\"start\"], opinion_item[\"end\"])\n",
    "        triplets.append((aspect_indices, opinion_indices, \"POS\"))\n",
    "    \n",
    "    # Format the triplets as a string (Python literal format).\n",
    "    triplets_str = str(triplets)\n",
    "    # Build the output line using the separator \"#### #### ####\".\n",
    "    output_line = tweet_text + \"#### #### ####\" + triplets_str\n",
    "    return output_line\n",
    "\n",
    "def convert_json_to_spanaste_format(json_file, output_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for item in data:\n",
    "            line = process_tweet(item)\n",
    "            fout.write(line + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_json = \"TJ - self_training/batch 5/batch 5.json\"  # your JSON file\n",
    "    output_txt = \"TJ - self_training//batch 5/spanaste_format.txt\"  # desired output filename\n",
    "    convert_json_to_spanaste_format(input_json, output_txt)\n",
    "    print(\"Conversion complete. Output saved to\", output_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
