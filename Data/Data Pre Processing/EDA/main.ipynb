{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ece2b8",
   "metadata": {},
   "source": [
    "## For EDA (Step 2: remove duplicate and EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5113eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 2522\n",
      "Total number of triplets: 3676\n",
      "\n",
      "Aspect word count distribution:\n",
      "  1 kata: 2191 label\n",
      "  2 kata: 1227 label\n",
      "  3 kata: 190 label\n",
      "  4 kata: 49 label\n",
      "  5 kata: 11 label\n",
      "  6 kata: 4 label\n",
      "  7 kata: 3 label\n",
      "  8 kata: 1 label\n",
      "\n",
      "Opinion word count distribution:\n",
      "  1 kata: 1291 label\n",
      "  2 kata: 1763 label\n",
      "  3 kata: 395 label\n",
      "  4 kata: 147 label\n",
      "  5 kata: 51 label\n",
      "  6 kata: 19 label\n",
      "  7 kata: 4 label\n",
      "  8 kata: 5 label\n",
      "  9 kata: 1 label\n",
      "\n",
      "Sentiment distribution:\n",
      "  NEG: 1583\n",
      "  POS: 1058\n",
      "  NEU: 1035\n",
      "\n",
      "Triplet distribution:\n",
      "  1 label: 1686 rows\n",
      "  2 labels: 615 rows\n",
      "  3 labels: 151 rows\n",
      "  4 labels: 52 rows\n",
      "  5 labels: 12 rows\n",
      "  6 labels: 4 rows\n",
      "  7 labels: 1 rows\n",
      "  8 labels: 1 rows\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "# Define the file name (adjust the path if necessary)\n",
    "filename = '../Data Labeling/TJ - Overall Dataset/v1.9 (self_training batch 4)/ori.txt'\n",
    "# filename = '../Data Labeling/TJ v1.8 (TwitterIO) Negative - v1.0/filtered_output.txt'\n",
    "# filename = '../Data Labeling/TJ - Overall Dataset/combined_dataset_max2tok.txt'\n",
    "# filename = '../Data Labeling/TJ - self_training/batch 5/filtered_output.txt'\n",
    "\n",
    "# Initialize counters for aspect and opinion word counts\n",
    "aspect_word_counts = Counter()\n",
    "opinion_word_counts = Counter()\n",
    "\n",
    "row_count = 0\n",
    "triplet_count = 0\n",
    "sentiment_counter = Counter()\n",
    "triplet_count_dist = Counter()\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        row_count += 1\n",
    "\n",
    "        # Split into tweet text and annotation\n",
    "        try:\n",
    "            _, annotation = line.rsplit(\"#### #### ####\", 1)\n",
    "        except ValueError:\n",
    "            print(f\"Line {row_count} does not have the expected format.\")\n",
    "            continue\n",
    "\n",
    "        # Parse annotations\n",
    "        try:\n",
    "            annotations = ast.literal_eval(annotation.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating annotations on line {row_count}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Count this row's triplets\n",
    "        n = len(annotations)\n",
    "        triplet_count += n\n",
    "        triplet_count_dist[n] += 1\n",
    "\n",
    "        # Count sentiment labels and word counts for aspect and opinion\n",
    "        for triplet in annotations:\n",
    "            if len(triplet) == 3:\n",
    "                aspect, opinion, sentiment = triplet\n",
    "                sentiment_counter[sentiment] += 1\n",
    "\n",
    "                # Count words in aspect\n",
    "                if isinstance(aspect, str):\n",
    "                    aspect_words = aspect.split()\n",
    "                    aspect_word_counts[len(aspect_words)] += 1\n",
    "                elif isinstance(aspect, list):\n",
    "                    aspect_word_counts[len(aspect)] += 1\n",
    "                else:\n",
    "                    print(f\"Warning: Aspect on line {row_count} is not str or list: {type(aspect)}\")\n",
    "\n",
    "                # Count words in opinion\n",
    "                if isinstance(opinion, str):\n",
    "                    opinion_words = opinion.split()\n",
    "                    opinion_word_counts[len(opinion_words)] += 1\n",
    "                elif isinstance(opinion, list):\n",
    "                    opinion_word_counts[len(opinion)] += 1\n",
    "                else:\n",
    "                    print(f\"Warning: Opinion on line {row_count} is not str or list: {type(opinion)}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"Total number of rows:\", row_count)\n",
    "print(\"Total number of triplets:\", triplet_count)\n",
    "\n",
    "print(\"\\nAspect word count distribution:\")\n",
    "for count in sorted(aspect_word_counts.keys()):\n",
    "    print(f\"  {count} kata: {aspect_word_counts[count]} label\")\n",
    "\n",
    "print(\"\\nOpinion word count distribution:\")\n",
    "for count in sorted(opinion_word_counts.keys()):\n",
    "    print(f\"  {count} kata: {opinion_word_counts[count]} label\")\n",
    "\n",
    "print(\"\\nSentiment distribution:\")\n",
    "for sentiment, count in sentiment_counter.items():\n",
    "    print(f\"  {sentiment}: {count}\")\n",
    "\n",
    "print(\"\\nTriplet distribution:\")\n",
    "for n in sorted(triplet_count_dist):\n",
    "    label = \"label\" if n == 1 else \"labels\"\n",
    "    print(f\"  {n} {label}: {triplet_count_dist[n]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73b4e567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet unik telah disimpan di file '../Data Labeling/TJ - Overall Dataset/v1.10 (self_training batch 5)/ori_removed duplicates.txt'\n",
      "Jumlah tweet duplikat yang ditemukan: 0\n"
     ]
    }
   ],
   "source": [
    "# remove duplicate tweets\n",
    "\n",
    "def hapus_tweet_duplikat(input_file):\n",
    "    \"\"\"\n",
    "    Menghapus tweet duplikat dari file input dan menyimpan hanya tweet pertama.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path ke file input.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple berisi list tweet unik dan jumlah tweet duplikat.\n",
    "    \"\"\"\n",
    "    tweet_unik = []\n",
    "    seen_tweets = set()\n",
    "    jumlah_duplikat = 0\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Ambil bagian tweet saja, sebelum metadata [([..])]\n",
    "            tweet = line.split('####')[0].strip()\n",
    "            if tweet not in seen_tweets:\n",
    "                tweet_unik.append(line.strip())\n",
    "                seen_tweets.add(tweet)\n",
    "            else:\n",
    "                jumlah_duplikat += 1\n",
    "    return tweet_unik, jumlah_duplikat\n",
    "\n",
    "def simpan_tweet_unik(output_file, list_tweet_unik):\n",
    "    \"\"\"\n",
    "    Menyimpan list tweet unik ke dalam file output.\n",
    "\n",
    "    Args:\n",
    "        output_file (str): Path ke file output.\n",
    "        list_tweet_unik (list): List berisi tweet unik.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tweet in list_tweet_unik:\n",
    "            f.write(tweet + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = \"../Data Labeling/TJ - Overall Dataset/v1.10 (self_training batch 5)/ori.txt\"\n",
    "    output_filename = \"../Data Labeling/TJ - Overall Dataset/v1.10 (self_training batch 5)/ori_removed duplicates.txt\"\n",
    "    tweet_unik, jumlah_duplikat = hapus_tweet_duplikat(input_filename)\n",
    "    simpan_tweet_unik(output_filename, tweet_unik)\n",
    "    print(f\"Tweet unik telah disimpan di file '{output_filename}'\")\n",
    "    print(f\"Jumlah tweet duplikat yang ditemukan: {jumlah_duplikat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "911342fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1:   96 baris  →  ..\\Data Labeling\\TJ v1.5 - v1.0.3\\scenario_splits\\filtered_output_removed duplicates_S1.txt\n",
      "S2:   19 baris  →  ..\\Data Labeling\\TJ v1.5 - v1.0.3\\scenario_splits\\filtered_output_removed duplicates_S2.txt\n",
      "S3:  520 baris  →  ..\\Data Labeling\\TJ v1.5 - v1.0.3\\scenario_splits\\filtered_output_removed duplicates_S3.txt\n",
      "✅  Selesai.  S4 = sisa baris di dataset asli.\n"
     ]
    }
   ],
   "source": [
    "# split dataset ke 4 skenario\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Pisahkan korpus Span-ASTE menjadi 4 skenario:\n",
    "S1 = 1 label, 1-kata ; S2 = multi label, 1-kata ;\n",
    "S3 = 1 label, multi-kata ; S4 = multi label, multi-kata (korpus penuh).\n",
    "\"\"\"\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "INFILE  = Path(\"../Data Labeling/TJ v1.5 - v1.0.3/filtered_output_removed duplicates.txt\")\n",
    "\n",
    "# ------- helper ----------------------------------------------------------------\n",
    "def parse_line(raw: str):\n",
    "    \"\"\"return text, list-of-triplets  (triplet = ([idx_aspek], [idx_opini], sent))\"\"\"\n",
    "    txt, ann = raw.rsplit(\"#### #### ####\", 1)\n",
    "    triplets = ast.literal_eval(ann.strip())\n",
    "    return txt.strip(), triplets\n",
    "\n",
    "def is_single_word(span):          # True jika panjang daftar index == 1\n",
    "    return len(span) == 1\n",
    "\n",
    "def triplet_all_single_word(trs):  # semua aspek & opini di tweet = 1 kata\n",
    "    return all(is_single_word(t[0]) and is_single_word(t[1]) for t in trs)\n",
    "\n",
    "def tweet_multi_word(trs):         # setidaknya satu aspek/opini > 1 kata\n",
    "    return any(len(t[0]) > 1 or len(t[1]) > 1 for t in trs)\n",
    "\n",
    "# ------- load & bucket ----------------------------------------------------------\n",
    "buckets = {     # tampung string baris\n",
    "    \"S1\": [],   # 1 label, single-word\n",
    "    \"S2\": [],   # multi label, single-word\n",
    "    \"S3\": []    # 1 label, multi-word\n",
    "    # S4 = sisanya\n",
    "}\n",
    "\n",
    "with INFILE.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        text, trips = parse_line(line)\n",
    "        n_label     = len(trips)\n",
    "\n",
    "        if n_label == 1 and triplet_all_single_word(trips):\n",
    "            buckets[\"S1\"].append(line)\n",
    "\n",
    "        elif n_label > 1 and triplet_all_single_word(trips):\n",
    "            buckets[\"S2\"].append(line)\n",
    "\n",
    "        elif n_label == 1 and tweet_multi_word(trips):\n",
    "            buckets[\"S3\"].append(line)\n",
    "\n",
    "        # else: otomatis masuk skenario 4 (multi-label & ada multi-kata)\n",
    "\n",
    "# ------- write out 3 skenario pertama ------------------------------------------\n",
    "OUT_DIR = INFILE.parent / \"scenario_splits\"\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "for tag, rows in buckets.items():\n",
    "    out_path = OUT_DIR / f\"{INFILE.stem}_{tag}.txt\"\n",
    "    out_path.write_text(\"\\n\".join(rows), encoding=\"utf-8\")\n",
    "    print(f\"{tag}: {len(rows):>4} baris  →  {out_path}\")\n",
    "\n",
    "print(\"✅  Selesai.  S4 = sisa baris di dataset asli.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2641560d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Merged file saved to: ../Data Labeling/TJ v1.5 - v1.0/merged_filtered_output.txt\n",
      "\n",
      "--- Summary ---\n",
      "Total number of rows: 771\n",
      "Total number of triplets: 1059\n",
      "\n",
      "Sentiment distribution:\n",
      "  NEU: 353\n",
      "  POS: 300\n",
      "  NEG: 406\n",
      "\n",
      "Triplet distribution:\n",
      "  1 label: 547 rows\n",
      "  2 labels: 182 rows\n",
      "  3 labels: 29 rows\n",
      "  4 labels: 8 rows\n",
      "  5 labels: 3 rows\n",
      "  6 labels: 1 rows\n",
      "  8 labels: 1 rows\n"
     ]
    }
   ],
   "source": [
    "# version 1.2\n",
    "\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "# Define the input files\n",
    "filenames = [\n",
    "    '../Data Labeling/TJ v1.5 - v1.1/filtered_output.txt',\n",
    "    '../Data Labeling/TJ v1.4 - v1.6/filtered_output.txt'\n",
    "]\n",
    "\n",
    "# Define the output merged file\n",
    "output_filename = '../Data Labeling/TJ v1.5 - v1.0/merged_filtered_output.txt'\n",
    "\n",
    "# Initialize counters\n",
    "row_count = 0\n",
    "triplet_count = 0\n",
    "sentiment_counter = Counter()\n",
    "triplet_count_dist = Counter()\n",
    "\n",
    "# Collect all lines first\n",
    "merged_lines = []\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            row_count += 1\n",
    "            merged_lines.append(line)  # Save the line for output\n",
    "\n",
    "            # Split into tweet text and annotation\n",
    "            try:\n",
    "                _, annotation = line.rsplit(\"#### #### ####\", 1)\n",
    "            except ValueError:\n",
    "                print(f\"Line {row_count} in {filename} does not have the expected format.\")\n",
    "                continue\n",
    "\n",
    "            # Parse annotations\n",
    "            try:\n",
    "                annotations = ast.literal_eval(annotation.strip())\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating annotations on line {row_count} in {filename}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Count this row's triplets\n",
    "            n = len(annotations)\n",
    "            triplet_count += n\n",
    "            triplet_count_dist[n] += 1\n",
    "\n",
    "            # Count sentiment labels\n",
    "            for triplet in annotations:\n",
    "                if len(triplet) == 3:\n",
    "                    sentiment = triplet[2]\n",
    "                    sentiment_counter[sentiment] += 1\n",
    "\n",
    "# --- Save merged file ---\n",
    "with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
    "    for line in merged_lines:\n",
    "        outfile.write(line + '\\n')\n",
    "\n",
    "print(f\"\\n✅ Merged file saved to: {output_filename}\")\n",
    "\n",
    "# --- Print summary ---\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(\"Total number of rows:\", row_count)\n",
    "print(\"Total number of triplets:\", triplet_count)\n",
    "print(\"\\nSentiment distribution:\")\n",
    "for sentiment, count in sentiment_counter.items():\n",
    "    print(f\"  {sentiment}: {count}\")\n",
    "\n",
    "print(\"\\nTriplet distribution:\")\n",
    "for n in sorted(triplet_count_dist):\n",
    "    label = \"label\" if n == 1 else \"labels\"\n",
    "    print(f\"  {n} {label}: {triplet_count_dist[n]} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dddab7d",
   "metadata": {},
   "source": [
    "## For split the dataset (Step 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec8232c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows loaded: 1213\n",
      "Training set size: 849\n",
      "Dev set size: 182\n",
      "Test set size: 182\n",
      "Train label distribution:\n",
      "  NEG: 273\n",
      "  NEU: 310\n",
      "  POS: 266\n",
      "Dev label distribution:\n",
      "  NEU: 66\n",
      "  NEG: 59\n",
      "  POS: 57\n",
      "Test label distribution:\n",
      "  NEG: 59\n",
      "  NEU: 66\n",
      "  POS: 57\n"
     ]
    }
   ],
   "source": [
    "# version 1.0\n",
    "\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def read_and_assign_labels(filename):\n",
    "    \"\"\"\n",
    "    Reads the annotated file and determines a dominant sentiment label for each line.\n",
    "    Each line is expected to have a tweet text and an annotation list separated by\n",
    "    \"#### #### ####\". The dominant label is computed as the most frequent sentiment\n",
    "    among the annotation triplets.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    dominant_labels = []\n",
    "    \n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for idx, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                # Expecting two parts: tweet text and annotation list\n",
    "                text_part, annotation_str = line.rsplit(\"#### #### ####\", 1)\n",
    "            except ValueError:\n",
    "                print(f\"Skipping line {idx} due to unexpected format.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Safely parse the annotation string into a Python object\n",
    "                annotations = ast.literal_eval(annotation_str.strip())\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing annotations on line {idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Extract the sentiment label (third element) from each annotation triplet\n",
    "            labels = [triplet[2] for triplet in annotations if len(triplet) == 3]\n",
    "            if labels:\n",
    "                dominant = Counter(labels).most_common(1)[0][0]\n",
    "            else:\n",
    "                dominant = None\n",
    "            data.append(line)\n",
    "            dominant_labels.append(dominant)\n",
    "    return data, dominant_labels\n",
    "\n",
    "# Read data from file and assign dominant labels for stratification.\n",
    "data, dominant_labels = read_and_assign_labels(\"../Data Labeling/TJ - Overall Dataset/v1.8 (max2tok)/merged_max2tok.txt\")\n",
    "print(\"Total rows loaded:\", len(data))\n",
    "\n",
    "# Split the data ensuring that the label proportions are roughly maintained.\n",
    "# First, split off a test set (15% of the data)\n",
    "train_dev_data, test_data, train_dev_labels, test_labels = train_test_split(\n",
    "    data, dominant_labels, test_size=0.15, random_state=42, stratify=dominant_labels)\n",
    "\n",
    "# Then, split the remaining train_dev set into training and development.\n",
    "# Here, we compute dev as roughly 15% of the entire dataset,\n",
    "# which is about 15% / 85% ≈ 17.65% of the train_dev set.\n",
    "train_data, dev_data, train_labels, dev_labels = train_test_split(\n",
    "    train_dev_data, train_dev_labels, test_size=0.1765, random_state=42, stratify=train_dev_labels)\n",
    "\n",
    "print(\"Training set size:\", len(train_data))\n",
    "print(\"Dev set size:\", len(dev_data))\n",
    "print(\"Test set size:\", len(test_data))\n",
    "\n",
    "def print_label_distribution(labels, split_name):\n",
    "    counter = Counter(lab for lab in labels if lab is not None)\n",
    "    print(f\"{split_name} label distribution:\")\n",
    "    for lab, count in counter.items():\n",
    "        print(f\"  {lab}: {count}\")\n",
    "\n",
    "print_label_distribution(train_labels, \"Train\")\n",
    "print_label_distribution(dev_labels, \"Dev\")\n",
    "print_label_distribution(test_labels, \"Test\")\n",
    "\n",
    "# Write the splits to separate text files.\n",
    "with open(\"../Data Labeling/TJ - Overall Dataset/v1.8 (max2tok)/train.txt\", \"w\", encoding='utf-8') as train_file:\n",
    "    for line in train_data:\n",
    "        train_file.write(line + \"\\n\")\n",
    "\n",
    "with open(\"../Data Labeling/TJ - Overall Dataset/v1.8 (max2tok)/dev.txt\", \"w\", encoding='utf-8') as dev_file:\n",
    "    for line in dev_data:\n",
    "        dev_file.write(line + \"\\n\")\n",
    "\n",
    "with open(\"../Data Labeling/TJ - Overall Dataset/v1.8 (max2tok)/test.txt\", \"w\", encoding='utf-8') as test_file:\n",
    "    for line in test_data:\n",
    "        test_file.write(line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aabb43d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows loaded: 3179\n",
      "Training set size: 2225\n",
      "Dev set size: 477\n",
      "Test set size: 477\n",
      "Train label distribution:\n",
      "  NEG: 996\n",
      "  POS: 555\n",
      "  NEU: 674\n",
      "Dev label distribution:\n",
      "  POS: 119\n",
      "  NEG: 213\n",
      "  NEU: 145\n",
      "Test label distribution:\n",
      "  POS: 119\n",
      "  NEU: 145\n",
      "  NEG: 213\n",
      "✅ Data successfully split into train/dev/test!\n"
     ]
    }
   ],
   "source": [
    "# version 1.1 - Direct 70/15/15 split\n",
    "\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def read_and_assign_labels(filename):\n",
    "    data = []\n",
    "    dominant_labels = []\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for idx, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                text_part, annotation_str = line.rsplit(\"#### #### ####\", 1)\n",
    "            except ValueError:\n",
    "                print(f\"Skipping line {idx} due to unexpected format.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                annotations = ast.literal_eval(annotation_str.strip())\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing annotations on line {idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "            labels = [triplet[2] for triplet in annotations if len(triplet) == 3]\n",
    "            if labels:\n",
    "                dominant = Counter(labels).most_common(1)[0][0]\n",
    "            else:\n",
    "                dominant = None\n",
    "            data.append(line)\n",
    "            dominant_labels.append(dominant)\n",
    "    return data, dominant_labels\n",
    "\n",
    "# Path to your merged file\n",
    "filename = \"../Data Labeling/TJ - Overall Dataset/v1.10 (self_training batch 5)/ori.txt\"\n",
    "\n",
    "# Read\n",
    "data, dominant_labels = read_and_assign_labels(filename)\n",
    "print(\"Total rows loaded:\", len(data))\n",
    "\n",
    "# --- Direct 70% train, 15% dev, 15% test split ---\n",
    "\n",
    "# First split off test set (15%)\n",
    "train_dev_data, test_data, train_dev_labels, test_labels = train_test_split(\n",
    "    data, dominant_labels, test_size=0.15, random_state=42, stratify=dominant_labels)\n",
    "\n",
    "# Then split train/dev (15/85 ≈ 0.1765 of 85% = ~15% of total)\n",
    "train_data, dev_data, train_labels, dev_labels = train_test_split(\n",
    "    train_dev_data, train_dev_labels, test_size=0.1765, random_state=42, stratify=train_dev_labels)\n",
    "\n",
    "# Check\n",
    "print(\"Training set size:\", len(train_data))\n",
    "print(\"Dev set size:\", len(dev_data))\n",
    "print(\"Test set size:\", len(test_data))\n",
    "\n",
    "# Label distribution\n",
    "def print_label_distribution(labels, split_name):\n",
    "    counter = Counter(lab for lab in labels if lab is not None)\n",
    "    print(f\"{split_name} label distribution:\")\n",
    "    for lab, count in counter.items():\n",
    "        print(f\"  {lab}: {count}\")\n",
    "\n",
    "print_label_distribution(train_labels, \"Train\")\n",
    "print_label_distribution(dev_labels, \"Dev\")\n",
    "print_label_distribution(test_labels, \"Test\")\n",
    "\n",
    "# --- Save splits to file ---\n",
    "output_folder = \"../Data Labeling/TJ - Overall Dataset/v1.10 (self_training batch 5)/\"\n",
    "with open(output_folder + \"train.txt\", \"w\", encoding='utf-8') as f:\n",
    "    for line in train_data:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "with open(output_folder + \"dev.txt\", \"w\", encoding='utf-8') as f:\n",
    "    for line in dev_data:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "with open(output_folder + \"test.txt\", \"w\", encoding='utf-8') as f:\n",
    "    for line in test_data:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(\"✅ Data successfully split into train/dev/test!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e0bde",
   "metadata": {},
   "source": [
    "# (Step 1) Merge Txt Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "54a46673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Membaca file: ../Data Labeling/TJ - Overall Dataset\\NEG.txt\n",
      "Membaca file: ../Data Labeling/TJ - Overall Dataset\\NEU.txt\n",
      "Membaca file: ../Data Labeling/TJ - Overall Dataset\\POS.txt\n",
      "Membaca file: ../Data Labeling/TJ - Overall Dataset\\TJ v1.5 - v1.0.txt\n",
      "Membaca file: ../Data Labeling/TJ - Overall Dataset\\TJ v1.7 - v1.0.txt\n",
      "\n",
      "Menulis semua konten ke file: ../Data Labeling/TJ - Overall Dataset\\combined_dataset.txt\n",
      "\n",
      "✅ Semua file .txt berhasil digabungkan menjadi combined_dataset.txt di dalam folder yang sama.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def append_all_txt_files(folder_path, output_filename=\"combined_dataset.txt\"):\n",
    "    \"\"\"\n",
    "    Menggabungkan semua file .txt dalam folder tertentu menjadi satu file output.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path ke folder yang berisi file .txt.\n",
    "        output_filename (str): Nama file output yang akan dibuat.\n",
    "    \"\"\"\n",
    "    all_text = []\n",
    "    try:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                filepath = os.path.join(folder_path, filename)\n",
    "                print(f\"Membaca file: {filepath}\")\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    content = f.readlines()\n",
    "                    all_text.extend(content)\n",
    "        \n",
    "        output_filepath = os.path.join(folder_path, output_filename)\n",
    "        print(f\"\\nMenulis semua konten ke file: {output_filepath}\")\n",
    "        with open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
    "            outfile.writelines(all_text)\n",
    "        \n",
    "        print(f\"\\n✅ Semua file .txt berhasil digabungkan menjadi {output_filename} di dalam folder yang sama.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Folder '{folder_path}' tidak ditemukan.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Terjadi kesalahan: {e}\")\n",
    "\n",
    "# Ganti path folder di bawah ini dengan path yang sesuai di komputer Anda\n",
    "folder_path = \"../Data Labeling/TJ - Overall Dataset\"\n",
    "append_all_txt_files(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a51f81",
   "metadata": {},
   "source": [
    "# Debugging (Step 4: Check for Format Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd7ed7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baris dengan masalah delimiter ditemukan:\n",
      "Baris 1889: 'Busway bejir#### #### ####[([0], [1], 'NEG')]buat apa ada teknologi yg disebut melacak posisi anda live location buat apa kalo lo ga gunain jir pt_transjakarta#### #### ####[([9, 10], [15, 16], 'NEG')]' - Alasan: Jumlah pemisah tidak tepat (ditemukan 3, diharapkan 2)\n"
     ]
    }
   ],
   "source": [
    "def detect_delimiter_issues(filename, delimiter=\"#### #### ####\"):\n",
    "    \"\"\"\n",
    "    Mendeteksi baris dalam file yang memiliki masalah dengan delimiter yang ditentukan.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path ke file data training.\n",
    "        delimiter (str): Delimiter yang diharapkan.\n",
    "\n",
    "    Returns:\n",
    "        list: List berisi nomor baris dan konten baris yang memiliki masalah delimiter.\n",
    "    \"\"\"\n",
    "    delimiter_issues = []\n",
    "    row_number = 0\n",
    "\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                row_number += 1\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                parts = line.split(delimiter)\n",
    "\n",
    "                if len(parts) != 2:\n",
    "                    delimiter_issues.append((row_number, line, f\"Jumlah pemisah tidak tepat (ditemukan {len(parts)}, diharapkan 2)\"))\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' tidak ditemukan.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Terjadi kesalahan: {e}\")\n",
    "        return None\n",
    "\n",
    "    return delimiter_issues\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = '../Data Labeling/TJ - Overall Dataset/v1.10 (self_training batch 5)/filtered_output.txt'\n",
    "    delimiter_problems = detect_delimiter_issues(input_filename)\n",
    "\n",
    "    if delimiter_problems is not None:\n",
    "        if delimiter_problems:\n",
    "            print(\"Baris dengan masalah delimiter ditemukan:\")\n",
    "            for row_num, line_content, error_reason in delimiter_problems:\n",
    "                print(f\"Baris {row_num}: '{line_content}' - Alasan: {error_reason}\")\n",
    "        else:\n",
    "            print(\"Tidak ada baris dengan masalah delimiter ditemukan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a8566",
   "metadata": {},
   "source": [
    "# Filtered in Tweet dengan Opini dan Aspek yang Katanya maksimal 2 (Step 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82e6462b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "─── Selesai ───\n",
      "Baris masuk          : 2275\n",
      "Baris tertulis       : 1848\n",
      "Triplet sebelum      : 3378\n",
      "Triplet sesudah      : 2629\n",
      "Triplet ter-drop     : 749\n",
      "→ File output        : ..\\Data Labeling\\TJ - Overall Dataset\\v1.8 (max2tok)\\merged_max2tok.txt\n"
     ]
    }
   ],
   "source": [
    "import ast, pathlib\n",
    "\n",
    "# ----------------- path -----------------\n",
    "IN_FILE  = '../Data Labeling/TJ - Overall Dataset/v1.8 (max2tok)/merged.txt'\n",
    "OUT_FILE = pathlib.Path(IN_FILE).with_name('merged_max2tok.txt')\n",
    "\n",
    "# ----------------- helper ----------------\n",
    "def keep_triplet(triplet):\n",
    "    \"\"\"\n",
    "    triplet = ([a_idx...], [o_idx...], 'POL')   – return True\n",
    "              hanya jika len(aspect_idx) <=2  dan  len(opinion_idx) <=2\n",
    "    \"\"\"\n",
    "    if len(triplet) != 3:\n",
    "        return False\n",
    "    a_idx, o_idx, _ = triplet\n",
    "    return len(a_idx) <= 2 and len(o_idx) <= 2\n",
    "\n",
    "# ----------------- main ------------------\n",
    "total_lines   = kept_lines = 0\n",
    "total_trp_in  = total_trp_out = 0\n",
    "\n",
    "with open(IN_FILE, encoding='utf-8') as fin, \\\n",
    "     open(OUT_FILE, 'w',  encoding='utf-8') as fout:\n",
    "\n",
    "    for line in fin:\n",
    "        line = line.rstrip('\\n')\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        total_lines += 1\n",
    "        try:\n",
    "            text, ann_str = line.rsplit(\"#### #### ####\", 1)\n",
    "            triplets      = ast.literal_eval(ann_str.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Skip line {total_lines}: {e}\")\n",
    "            continue\n",
    "\n",
    "        total_trp_in += len(triplets)\n",
    "\n",
    "        # --- filter ---\n",
    "        triplets_kept = [t for t in triplets if keep_triplet(t)]\n",
    "        if triplets_kept:\n",
    "            kept_lines    += 1\n",
    "            total_trp_out += len(triplets_kept)\n",
    "            fout.write(f\"{text}#### #### ####{triplets_kept}\\n\")\n",
    "\n",
    "# ----------------- report ----------------\n",
    "print(\"─── Selesai ───\")\n",
    "print(f\"Baris masuk          : {total_lines}\")\n",
    "print(f\"Baris tertulis       : {kept_lines}\")\n",
    "print(f\"Triplet sebelum      : {total_trp_in}\")\n",
    "print(f\"Triplet sesudah      : {total_trp_out}\")\n",
    "print(f\"Triplet ter-drop     : {total_trp_in - total_trp_out}\")\n",
    "print(f\"→ File output        : {OUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8fa60c",
   "metadata": {},
   "source": [
    "# for Shuffle the train.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0792fafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 1817 baris dari '../Data Labeling/TJ - Overall Dataset/v1.6 (self_training batch 3)/train.txt' sudah diacak dan disimpan ke '../Data Labeling/TJ - Overall Dataset/v1.6 (self_training batch 3)/train_shuffled.txt'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "INPUT_FILE = \"../Data Labeling/TJ - Overall Dataset/v1.6 (self_training batch 3)/train.txt\"\n",
    "OUTPUT_FILE = \"../Data Labeling/TJ - Overall Dataset/v1.6 (self_training batch 3)/train_shuffled.txt\"\n",
    "\n",
    "# Baca semua baris\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Acak urutan\n",
    "random.shuffle(lines)\n",
    "\n",
    "# Tulis kembali ke file baru\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(lines)\n",
    "\n",
    "print(f\"✅ {len(lines)} baris dari '{INPUT_FILE}' sudah diacak dan disimpan ke '{OUTPUT_FILE}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
