{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca0dba6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\KULIAH\\Semester 7\\Tugas Akhir\\Crawl Data\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'id_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOC_Merged_deduplicated_keyword filtered_cleaned_output_single word real.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 2. Lemmatization + Lowercasing\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid_core_news_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnormalize_lemma\u001b[39m(text):\n\u001b[0;32m     12\u001b[0m     doc \u001b[38;5;241m=\u001b[39m nlp(text)\n",
      "File \u001b[1;32md:\\KULIAH\\Semester 7\\Tugas Akhir\\Crawl Data\\venv\\Lib\\site-packages\\spacy\\__init__.py:52\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     29\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     36\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\KULIAH\\Semester 7\\Tugas Akhir\\Crawl Data\\venv\\Lib\\site-packages\\spacy\\util.py:484\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 484\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'id_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 1. Load data\n",
    "df = pd.read_csv(\"POC_Merged_deduplicated_keyword filtered_cleaned_output_single word real.csv\")\n",
    "\n",
    "# 2. Lemmatization + Lowercasing\n",
    "nlp = spacy.load(\"id_core_news_sm\", disable=[\"parser\",\"ner\"])\n",
    "def normalize_lemma(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join(tok.lemma_.lower() for tok in doc)\n",
    "\n",
    "df[\"target_lemma\"] = df[\"target\"].astype(str).apply(normalize_lemma)\n",
    "\n",
    "# 3. Mapping dengan kamus kecil (bisa diisi manual dari top-aspek)\n",
    "mapping = {\n",
    "    \"busway\": \"transjakarta\",\n",
    "    \"tj\": \"transjakarta\",\n",
    "    \"transjakarta\": \"transjakarta\",\n",
    "    \"jalur\": \"rute\",\n",
    "    \"jalurnya\": \"rute\",\n",
    "    \"nunggu\": \"headway\",\n",
    "    \"nungguin\": \"headway\",\n",
    "    \"transum\": \"transjakarta\",\n",
    "    \"bus\": \"armada\",\n",
    "    \"jaklingko\": \"jaklingko\",\n",
    "    \"jalurnya\": \"rute\",\n",
    "    \"tarif\": \"tarif\",\n",
    "    \"ac\": \"ac\",\n",
    "    \"angkot\": \"jaklingko\",\n",
    "    \"armada\": \"armada\",\n",
    "    \"rutenya\": \"rute\",\n",
    "    \"transportasi\": \"transjakarta\",\n",
    "    \"macetnya\": \"lalu lintas\",\n",
    "    \"antrian\": \"antrian\",\n",
    "    \"kartu\": \"pembayaran\",\n",
    "    \"petugas\": \"pramusapa\",\n",
    "    \"akses\": \"aksesibilitas\",\n",
    "    \"bis\": \"armada\",\n",
    "    \"saldo\": \"pembayaran\",\n",
    "    \"layanan\": \"layanan\",\n",
    "    \"nungguin\": \"headway\",\n",
    "    \"nunggunya\": \"headway\",\n",
    "    \"transumnya\": \"transjakarta\",\n",
    "    \"supir\": \"pramudi\",\n",
    "    \"aplikasi\": \"aplikasi\",\n",
    "    \"datengnya\": \"headway\",\n",
    "    \"lift\": \"lift\",\n",
    "    \"pt_transjakarta\": \"transjakarta\",\n",
    "    \"ngantri\": \"antrian\",\n",
    "    \"supirnya\": \"pramudi\",\n",
    "    \"tjnya\": \"armada\",\n",
    "    # tambahkan jika perlu...\n",
    "}\n",
    "\n",
    "df[\"target_mapped\"] = df[\"target_lemma\"].map(mapping).fillna(df[\"target_lemma\"])\n",
    "\n",
    "# 4. (Opsional) Clustering embeddings untuk grouping otomatis\n",
    "#    Jalankan hanya sekali jika daftar unique target masih besar\n",
    "unique_targets = df[\"target_mapped\"].unique().tolist()\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "embeds = model.encode(unique_targets)\n",
    "\n",
    "# Misal kita ingin ~15 cluster:\n",
    "n_clusters = 15\n",
    "km = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "labels = km.fit_predict(embeds)\n",
    "\n",
    "# Buat DataFrame cluster untuk review manual\n",
    "cluster_df = pd.DataFrame({\n",
    "    \"target\": unique_targets,\n",
    "    \"cluster\": labels\n",
    "})\n",
    "\n",
    "# Simpan untuk diâ€“inspect: beri label general per cluster secara manual\n",
    "cluster_df.to_csv(\"clusters_to_label.csv\", index=False)\n",
    "\n",
    "# Setelah Anda meninjau clusters_to_label.csv dan membuat dict `cluster_label = {cluster_id: \"nama_general\"...}`\n",
    "# Anda bisa mapping:\n",
    "# cluster_label = {0: \"halte\", 1: \"saldo\", ...}\n",
    "# df[\"target_final\"] = df[\"target_mapped\"].map(\n",
    "#     lambda t: cluster_label[km.predict([model.encode(t)])[0]]\n",
    "# )\n",
    "\n",
    "# 5. Simpan hasil\n",
    "# df.to_csv(\"predictions_normalized.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e194d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import defaultdict\n",
    "\n",
    "# Contoh data (ganti dengan CSV Anda)\n",
    "data = 'POC_Merged_deduplicated_keyword filtered_cleaned_output_single word real.csv'\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. LEXICON-BASED NORMALIZATION\n",
    "normalization_dict = {\n",
    "    \"busway\": \"transjakarta\",\n",
    "    \"tj\": \"transjakarta\",\n",
    "    \"transjakarta\": \"transjakarta\",\n",
    "    \"jalur\": \"rute\",\n",
    "    \"jalurnya\": \"rute\",\n",
    "    \"nunggu\": \"headway\",\n",
    "    \"nungguin\": \"headway\",\n",
    "    \"transum\": \"transjakarta\",\n",
    "    \"bus\": \"armada\",\n",
    "    \"jaklingko\": \"jaklingko\",\n",
    "    \"jalurnya\": \"rute\",\n",
    "    \"tarif\": \"tarif\",\n",
    "    \"ac\": \"ac\",\n",
    "    \"angkot\": \"jaklingko\",\n",
    "    \"armada\": \"armada\",\n",
    "    \"rutenya\": \"rute\",\n",
    "    \"transportasi\": \"transjakarta\",\n",
    "    \"macetnya\": \"lalu lintas\",\n",
    "    \"antrian\": \"antrian\",\n",
    "    \"kartu\": \"pembayaran\",\n",
    "    \"petugas\": \"pramusapa\",\n",
    "    \"akses\": \"aksesibilitas\",\n",
    "    \"bis\": \"armada\",\n",
    "    \"saldo\": \"pembayaran\",\n",
    "    \"layanan\": \"layanan\",\n",
    "    \"nungguin\": \"headway\",\n",
    "    \"nunggunya\": \"headway\",\n",
    "    \"transumnya\": \"transjakarta\",\n",
    "    \"supir\": \"pramudi\",\n",
    "    \"aplikasi\": \"aplikasi\",\n",
    "    \"datengnya\": \"headway\",\n",
    "    \"lift\": \"lift\",\n",
    "    \"pt_transjakarta\": \"transjakarta\",\n",
    "    \"ngantri\": \"antrian\",\n",
    "    \"supirnya\": \"pramudi\",\n",
    "    \"tjnya\": \"armada\",\n",
    "\n",
    "}\n",
    "\n",
    "# Fungsi normalisasi lexicon\n",
    "def lexicon_normalize(term):\n",
    "    term_lower = term.lower()\n",
    "    return normalization_dict.get(term_lower, term)\n",
    "\n",
    "# 2. CLUSTERING UNKNOWN TERMS (Untuk istilah yang belum dinormalisasi)\n",
    "# Load pretrained Indonesian word embeddings (download dulu dari https://fasttext.cc/docs/en/crawl-vectors.html)\n",
    "# model = KeyedVectors.load_word2vec_format('cc.id.300.vec', limit=100000)\n",
    "\n",
    "# Simulasi embedding (gunakan model nyata untuk implementasi)\n",
    "def get_cluster(term):\n",
    "    # Contoh: clustering manual berdasarkan contoh data\n",
    "    if term.lower() in ['tasnya', 'madu']:\n",
    "        return 'Fasilitas'\n",
    "    return 'Lainnya'\n",
    "\n",
    "# Gabungkan kedua metode\n",
    "def normalize_aspect(term):\n",
    "    # Coba normalisasi lexicon dulu\n",
    "    normalized = lexicon_normalize(term)\n",
    "    if normalized != term:  # Jika sudah dinormalisasi\n",
    "        return normalized\n",
    "    # Jika tidak, gunakan clustering\n",
    "    return get_cluster(term)\n",
    "\n",
    "# Terapkan ke dataframe\n",
    "df['normalized_target'] = df['target'].apply(normalize_aspect)\n",
    "\n",
    "print(df[['target', 'normalized_target']].drop_duplicates())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
